{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# download and transform train dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#download and transform test dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple convnet classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        \n",
    "        # Do not be afraid of F's - those are just functional wrappers for modules form nn package\n",
    "        # Please, see for yourself - http://pytorch.org/docs/_modules/torch/nn/functional.html\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 320 -> 50\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # 50 -> 10\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # transform to logits\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create classifier and optimizer objects\n",
    "clf = CNNClassifier()\n",
    "opt = optim.SGD(clf.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "    \n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target = Variable(label)\n",
    "        \n",
    "        # forward pass, calculate loss and backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = F.nll_loss(preds, target)\n",
    "        loss.backward()\n",
    "        loss_history.append(loss.data[0])\n",
    "        opt.step()\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print(loss.data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data, volatile=True) \n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = clf(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "2.2804977893829346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\envs\\ai-sat\\lib\\site-packages\\ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1148722171783447\n",
      "1.604717493057251\n",
      "1.266524076461792\n",
      "0.9379181861877441\n",
      "0.5698641538619995\n",
      "0.9099873304367065\n",
      "1.178101658821106\n",
      "0.5335041284561157\n",
      "0.694959282875061\n",
      "1.2525625228881836\n",
      "0.3119519352912903\n",
      "0.8239191770553589\n",
      "0.3108687400817871\n",
      "0.1670413315296173\n",
      "0.7176213264465332\n",
      "0.09575709700584412\n",
      "0.5189855098724365\n",
      "0.3324759900569916\n",
      "0.4430099129676819\n",
      "0.33492276072502136\n",
      "0.4508073329925537\n",
      "0.7515448927879333\n",
      "0.25965601205825806\n",
      "0.7030712366104126\n",
      "0.22288446128368378\n",
      "0.3731037974357605\n",
      "0.1024196594953537\n",
      "0.1399500072002411\n",
      "0.07810962945222855\n",
      "0.6492373943328857\n",
      "0.5064980983734131\n",
      "0.7285563349723816\n",
      "0.5071563124656677\n",
      "0.8008257150650024\n",
      "0.956944465637207\n",
      "0.47253546118736267\n",
      "0.08557847887277603\n",
      "0.05572452396154404\n",
      "0.5363484025001526\n",
      "1.041324257850647\n",
      "0.14581237733364105\n",
      "0.08092140406370163\n",
      "0.062430549412965775\n",
      "0.15988200902938843\n",
      "0.9279142618179321\n",
      "0.008257364854216576\n",
      "0.3426084816455841\n",
      "0.6280678510665894\n",
      "0.9156116247177124\n",
      "0.30680862069129944\n",
      "0.22146566212177277\n",
      "0.8154104948043823\n",
      "0.05352471023797989\n",
      "0.1306760311126709\n",
      "0.10054762661457062\n",
      "0.06936363875865936\n",
      "0.19052498042583466\n",
      "0.38821864128112793\n",
      "0.3927071690559387\n",
      "\n",
      "Test set: Average loss: 0.0830, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "Epoch 1\n",
      "0.13595324754714966\n",
      "0.5574067831039429\n",
      "0.2769499123096466\n",
      "0.04427208751440048\n",
      "1.055375576019287\n",
      "0.26456618309020996\n",
      "0.8062035441398621\n",
      "0.5430234670639038\n",
      "0.28613197803497314\n",
      "0.010606219060719013\n",
      "0.170231893658638\n",
      "0.04627227783203125\n",
      "0.030651863664388657\n",
      "0.35947805643081665\n",
      "0.3600202202796936\n",
      "0.5181438326835632\n",
      "0.386247456073761\n",
      "0.5828937292098999\n",
      "0.027605075389146805\n",
      "0.06356339156627655\n",
      "0.3432082235813141\n",
      "1.0269855260849\n",
      "0.6663641929626465\n",
      "0.05148594453930855\n",
      "0.36657941341400146\n",
      "1.3227311372756958\n",
      "0.0006002368172630668\n",
      "0.24425816535949707\n",
      "0.1693999171257019\n",
      "0.103779137134552\n",
      "0.03850246220827103\n",
      "0.12875330448150635\n",
      "0.13610608875751495\n",
      "0.2697955369949341\n",
      "0.26406365633010864\n",
      "0.2296052724123001\n",
      "0.21145889163017273\n",
      "0.40326961874961853\n",
      "0.21470537781715393\n",
      "0.17316383123397827\n",
      "0.2599315345287323\n",
      "0.07818622887134552\n",
      "0.15321649610996246\n",
      "0.12610867619514465\n",
      "0.029603764414787292\n",
      "0.4373392164707184\n",
      "0.13701656460762024\n",
      "0.19389477372169495\n",
      "0.6346856951713562\n",
      "0.00762776518240571\n",
      "0.12917651236057281\n",
      "0.03393254801630974\n",
      "0.3882601857185364\n",
      "0.3190641701221466\n",
      "0.018664421513676643\n",
      "0.06603050976991653\n",
      "0.045852627605199814\n",
      "0.2376401126384735\n",
      "0.12385030835866928\n",
      "0.00744619220495224\n",
      "\n",
      "Test set: Average loss: 0.0649, Accuracy: 9795/10000 (98%)\n",
      "\n",
      "Epoch 2\n",
      "0.3336680829524994\n",
      "0.1902446746826172\n",
      "0.2344837635755539\n",
      "0.050005920231342316\n",
      "0.3328777849674225\n",
      "0.44579511880874634\n",
      "0.5638312101364136\n",
      "0.2656915783882141\n",
      "0.013651507906615734\n",
      "0.04496094211935997\n",
      "0.33629676699638367\n",
      "0.08216585218906403\n",
      "0.13246050477027893\n",
      "0.17158101499080658\n",
      "0.035118382424116135\n",
      "0.22898855805397034\n",
      "0.13093417882919312\n",
      "0.08415409177541733\n",
      "0.06062789633870125\n",
      "0.5523133873939514\n",
      "0.24881486594676971\n",
      "0.2766883969306946\n",
      "0.270297646522522\n",
      "0.0390598438680172\n",
      "0.3862993121147156\n",
      "0.3322864770889282\n",
      "0.01208521332591772\n",
      "0.9761684536933899\n",
      "0.12916378676891327\n",
      "0.009608296677470207\n",
      "0.051372844725847244\n",
      "0.23885805904865265\n",
      "0.016980711370706558\n",
      "0.5785446166992188\n",
      "0.16592325270175934\n",
      "0.1105828508734703\n",
      "0.11822332441806793\n",
      "0.18560758233070374\n",
      "0.3605871796607971\n",
      "0.17585380375385284\n",
      "0.06890088319778442\n",
      "0.015768641605973244\n",
      "0.0091382572427392\n",
      "0.21229812502861023\n",
      "0.48715895414352417\n",
      "0.02607230469584465\n",
      "0.0130159305408597\n",
      "0.09225257486104965\n",
      "0.07201585918664932\n",
      "0.10135278850793839\n",
      "0.21180351078510284\n",
      "0.0013576209312304854\n",
      "0.13260626792907715\n",
      "0.4564683437347412\n",
      "0.0033330630976706743\n",
      "0.05346863344311714\n",
      "0.2746652066707611\n",
      "0.13775958120822906\n",
      "0.03454151004552841\n",
      "0.05521325394511223\n",
      "\n",
      "Test set: Average loss: 0.0659, Accuracy: 9794/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 3):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
